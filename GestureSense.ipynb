{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a95478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade opencv-contrib-python== 4.10.0.84\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc933ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088361b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0ec489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "  \n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                handedness = results.multi_handedness[0].classification[0].label\n",
    "                if handedness == 'Right':\n",
    "  \n",
    "                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                    all_fingers_right = True\n",
    "                    all_fingers_left = True\n",
    "                    all_fingers_up = True\n",
    "                    all_fingers_down = True\n",
    "\n",
    "                    for id in range(5): \n",
    "      \n",
    "                        tip = hand_landmarks.landmark[mp_hands.HandLandmark(id * 4 + 4)]\n",
    "                        pip = hand_landmarks.landmark[mp_hands.HandLandmark(id * 4 + 2)]\n",
    "                        mcp = hand_landmarks.landmark[mp_hands.HandLandmark(id * 4 + 1)]\n",
    "                        \n",
    "                        if tip.x < pip.x:\n",
    "                            all_fingers_right = False\n",
    "\n",
    "                        if tip.x > pip.x:\n",
    "                            all_fingers_left = False\n",
    "\n",
    "                        if tip.y > pip.y:\n",
    "                            all_fingers_up = False\n",
    "                        \n",
    "                        if tip.y < mcp.y:\n",
    "                            all_fingers_down = False\n",
    "\n",
    "                    if all_fingers_right:\n",
    "                        gesture = \"Right\"\n",
    "                    elif all_fingers_left:\n",
    "                        gesture = \"Left\"\n",
    "                    elif all_fingers_up:\n",
    "                        gesture = \"Break\"\n",
    "                    elif all_fingers_down:\n",
    "                        gesture = \"MoveBack\"\n",
    "                    else:\n",
    "                        gesture = \"go\"\n",
    "\n",
    "                    if gesture:\n",
    "                        cv2.putText(frame, gesture, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow('Hand Gesture Recognition', frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4351296",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\Desktop\\FInal OCR\\OCR_Reader\\Lib\\site-packages\\google\\protobuf\\descriptor_pool.py:1354\u001b[0m, in \u001b[0;36mDefault\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1350\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1351\u001b[0m   _DEFAULT \u001b[38;5;241m=\u001b[39m DescriptorPool()\n\u001b[1;32m-> 1354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mDefault\u001b[39m():\n\u001b[0;32m   1355\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _DEFAULT\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "def is_finger_raised(hand_landmarks, finger_indices, finger_name, is_right_hand=True):\n",
    "    if finger_name == 'thumb':\n",
    "        if is_right_hand:\n",
    "            return hand_landmarks.landmark[finger_indices[1]].x > hand_landmarks.landmark[finger_indices[0]].x\n",
    "        else:\n",
    "            return hand_landmarks.landmark[finger_indices[1]].x < hand_landmarks.landmark[finger_indices[0]].x\n",
    "    else:\n",
    "        return hand_landmarks.landmark[finger_indices[1]].y < hand_landmarks.landmark[finger_indices[0]].y\n",
    "\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the image to RGB\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        output = 'other'\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                hand_label = handedness.classification[0].label\n",
    "                is_right_hand = hand_label == 'Right'\n",
    "\n",
    "                finger_indices = {\n",
    "                    'thumb': [mp_hands.HandLandmark.THUMB_MCP, mp_hands.HandLandmark.THUMB_TIP],\n",
    "                    'index': [mp_hands.HandLandmark.INDEX_FINGER_MCP, mp_hands.HandLandmark.INDEX_FINGER_TIP],\n",
    "                    'middle': [mp_hands.HandLandmark.MIDDLE_FINGER_MCP, mp_hands.HandLandmark.MIDDLE_FINGER_TIP],\n",
    "                    'ring': [mp_hands.HandLandmark.RING_FINGER_MCP, mp_hands.HandLandmark.RING_FINGER_TIP],\n",
    "                    'pinky': [mp_hands.HandLandmark.PINKY_MCP, mp_hands.HandLandmark.PINKY_TIP],\n",
    "                }\n",
    "\n",
    "                fingers_raised = {\n",
    "                    'thumb': is_finger_raised(hand_landmarks, finger_indices['thumb'], 'thumb', is_right_hand),\n",
    "                    'index': is_finger_raised(hand_landmarks, finger_indices['index'], 'index', is_right_hand),\n",
    "                    'middle': is_finger_raised(hand_landmarks, finger_indices['middle'], 'middle', is_right_hand),\n",
    "                    'ring': is_finger_raised(hand_landmarks, finger_indices['ring'], 'ring', is_right_hand),\n",
    "                    'pinky': is_finger_raised(hand_landmarks, finger_indices['pinky'], 'pinky', is_right_hand),\n",
    "                }\n",
    "                print(fingers_raised)\n",
    "\n",
    "                # Gesture detection logic (same as before)\n",
    "                if fingers_raised['index'] and not fingers_raised['middle'] and not fingers_raised['ring']and not fingers_raised['pinky']and not fingers_raised['thumb']:\n",
    "                    output = 1\n",
    "                elif fingers_raised['index'] and fingers_raised['middle'] and not fingers_raised['ring']and not fingers_raised['pinky']and not fingers_raised['thumb']:\n",
    "                    output = 2\n",
    "                elif fingers_raised['index'] and fingers_raised['middle'] and fingers_raised['ring']and not fingers_raised['pinky']and not fingers_raised['thumb']:\n",
    "                    output = 3\n",
    "                else:\n",
    "                    output = 'other'\n",
    "\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, \n",
    "                    hand_landmarks, \n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                    mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "        cv2.putText(frame, f'Gesture: {output}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(\"Hand Tracking\", frame)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OCR_Reader",
   "language": "python",
   "name": "ocr_reader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
